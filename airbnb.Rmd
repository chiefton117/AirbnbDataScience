---
title: "AirBNB Data Analysis"
author: "Tim Harrold, Madison Brown, Yifei Hao"
date: "3/10/2021"
output: 
  html_document:
      toc: true
      code_folding: hide
      df_print:  paged 
---


*NOTE: There will be a PDF and HTML document submitted for this project. The PDF contains everything significant in the report, but the HTML file will allow you to use the interactive leaflet.*

# Introduction

Our data was sourced from Airbnb's website: http://insideairbnb.com/get-the-data.html    

In this analysis we will be observing qualitative and quantitative characteristics to discover the variation in Airbnbs throughout New York City, and what makes the best Airbnb experience. We ask the question, which characteristics are correlated with a better reported experience?    

This analysis will include graphs to show the relationships between price and neighborhood, rating and neighborhood, rating and number of reviews, and rating and review stop words. We will be using tables, density plots, scatter plots, and maps to analyze the correlations between these variables. To give a qualitative analysis of peoples stays, we will be using word cloud to filter keywords from the reviews. A map of the city will provide spacial information on where the most vs least expensive Airbnbs are located in the city.

We don't believe in any sampling bias by Airbnb in this observational 'study', as this data is from an objective database of ALL airbnb listings. These listings included metadata that we're pulling from, such as price, reviews, and location. However, it is possible that the qualitative elements such as amenities may be biased, as these are up to the property owners to create. Some owners may oversell their listing. This bias hopefully will be overcome by the reviews of said listing. Analyzing more objective attributes may provide a clearer understanding of each property's offerings.     

There was a lot of data cleaning done to produce this data over many iterations. A private github repo / git log of all commits can be produced upon request. Some methods include:    

- Scrubbing language of all punctuation and non-alphanumeric characters using regular expressions
- Counting and analyzing attribute metadata such as length
- Counting and analyzing pertinent words and phrases
- Taking and modifying a random sample of data to lower computational cost while providing meaningful accuracy
- Removing extraneous values from graphs for easier interpretation     

This data is interesting because it is real. AirBNB is becoming a more viable alternative to hotels or motels because they are cheaper, cozier, and closer to the city experience than a hotel. Each experience is unique, making it hard to know exactly what a good AirBNB looks like. Finding these qualities can make your next vacation better, and expedite research on making good plans.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(devtools)
library(maps)
library(leaflet)
library(ggmap)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggwordcloud)
library(grid)
library(gridExtra)


listings <- read.csv('listings_NYC.csv');
reviews <- read.csv('reviews_NYC.csv');

# In the setup chunk, create a numeric casting of price for analysis in the rest of the code

listings$priceNum <- as.numeric(gsub('[$,]', '', listings$price))


```


# Data Visualization

## Mapped Listings by Price

*Here, we tried to utilize ggmaps and ggplot before resorting to leaflet for our map. I tried mapping the review scores, which didn't quite display right. Most scores are very high, even when mapping 68% of the data within one standard deviation of the mean.*

*The other challenge was taking a small enough subset of the data to map without crashing a computer(three computer crashes were involved in the making of this report). Leaflet offers multiple coloring methods, such as numerical, bin-based, and quantile-based. Quantiles ended up working especially well for this map.*

We hypothesized that the Bronx and Brooklyn would have a higher concentration of lower priced Airbnbs than Staten Island and Manhattan. Our hypothesis is only partially correct. Brooklyn has a similar distribution to Staten Island, and Queens has a similar distribution to the Bronx. Our hypothesis was supported in the case of the Bronx and Manhattan. 

This map also has some interesting findings in the density of properties, and their price. The listings get much more dense the further into Manhattan you go, second to Queens and Brooklyn. Interestingly, price also increases as you get into these areas. As per our earlier findings, Manhattan and Brooklyn are the most expensive boroughs to stay in.

Within Manhattan, values get more dense and more expensive towards the south end. This is the financial district of NYC, where most skyscrapers and businesses are. Perhaps people with financial interests, or wealthier individuals like Wall Street bankers stay in pricier AirBNB's. SoHo and Greenwich Village are also in this part of Manhattan, which must be more expensive.

When people visit NYC, they want to be closer to the center of the city. They likely pay more for staying in or near Manhattan - where most tourist attractions are. To be cliche, the saying is 'Location, Location, Location' in real estate.

```{r maps, force = TRUE, message=FALSE, warning=FALSE}

set.seed(200)
subset <- sample_n(listings, 3000)

stdev <- sd(subset$priceNum)
mean <- mean(subset$priceNum)


# This is within one standard deviation of the mean
sData <- subset %>% filter(priceNum > (mean - stdev) & priceNum < (mean + stdev))
# This is within two standard deviations of the mean
sd2Data <- subset %>% filter(priceNum > mean - (2*stdev) & priceNum < mean + (2*stdev))

clrs <- colorQuantile(palette = "RdYlBu", domain = sData$priceNum, reverse = TRUE, na.color = NA)

# plot price on map, not sure what the actual markers signify?
leaflet(sData) %>%
  addTiles() %>%
  addCircleMarkers(data = sData,
           lng = ~longitude,
           lat = ~latitude,
           color = ~clrs(priceNum),
           stroke = FALSE, fillOpacity = 0.4, radius = 4) %>%
  addLegend("bottomright", pal = clrs, values = ~priceNum,
    title = "Price(Quantile)",
    labFormat = labelFormat(prefix = ""),
    opacity = 1) %>%
  addMarkers( clusterOptions = markerClusterOptions() ) %>%
  setView(-74.00, 40.71, zoom = 12) %>%
  addProviderTiles("CartoDB.Positron")

```

## Box Plot of Price

This Box Plot offers an alternate view of Price from the map above. It is able to demonstrate the statistics for each borough while the map is case specific. While the map shows the viewer a physical representation of price, the box plot provides statistics for the viewer to analyze and compare across boroughs. The graph shows many outliers that may add a bias or skew the data. In Brooklyn and Manhattan especially, the data has a lot of points that are far above average. This could mean that analysis of this data may not reflect the average experience, but rather a small subset of very pricey listings.

```{r boxplot}
boxplot <- as.data.frame(sample_n(listings, 1500))

a1 <- ggplot(data = boxplot %>% filter(priceNum < 2000), 
       mapping = aes(x = neighbourhood_group_cleansed, y = priceNum, fill = neighbourhood_group_cleansed)) +
  geom_boxplot() +
  labs(title = 'Price by Neighborhood',
       subtitle = 'Price < 2000',
       x = 'Neighborhood',
       y = 'Price') +
  guides(fill = FALSE) +
  theme_bw()

a1 + theme(plot.title = element_text(size= 15, face = 'bold'))
  
```


## Density by Price

This graph shows the density of Airbnbs with different review scores within the five boroughs.  We hypothesized that the Bronx and Brooklyn would have the higher densities of lower reviewed Airbnbs than Staten Island and Manhattan. Our hypothesis is only partially correct. Brooklyn has a similar distribution to Manhattan. There are more Airbnbs in these boroughs with very high ratings than there are in the other boroughs. Our hypothesis was supported in the case of the Bronx, queens, and Staten Island. The distribution of ratings is similar among these boroughs. Manhattan and Brooklyn are more expensive and desirable boroughs for living and tourism. This not only increases the price, but may also affect quality. The quality of Airbnbs among the boroughs is likely reflected in the review scores. 


*We attempted to arrange the facet_wrap into one column to make reading the graphs and comparing the boroughs easier. This stretched out each graph and did not help with readability. It is easiest to read facet_wrap with three columns. 

```{r listings, warning = FALSE}

ggplot(data = listings, mapping = aes(x=review_scores_rating, fill=neighbourhood_group_cleansed)) +
  geom_density() +
  facet_wrap(~ neighbourhood_group_cleansed) +
  guides(fill = FALSE) +
  labs(title = "Airbnb Density by Review Score",
       x = 'Review Score',
       y = 'Density') +
  theme(plot.title = element_text(size= 15, face = 'bold')) +
  theme_bw() 
  


```


## Review Scores by Price

This graph shows the review score by price of Airbnbs in each borough. We hypothesized that there would be a positive correlation between price and review score. The data show that higher priced Airbnbs tend to have high review scores. Most of the multithousand dollar Airbnbs do not have review scores below 80. Although more expensive Airbnbs trend towards higher scores, this does not mean that cheaper Airbnbs trend towards lower scores. The Airbnbs with the lower scores tend to be the cheaper ones; However, less expensive Airbnbs also have high scores. From this, we conclude that price has less impact on cheaper Airbnb review scores than expensive ones. 

*scale_x_log10() was used to make the graphs more readable.

```{r scatter, warning = FALSE}

a1 <- ggplot(data = listings, mapping = aes(x=priceNum, y=review_scores_rating, color=neighbourhood_group_cleansed)) +
  geom_point() +
  scale_x_log10() +
  facet_wrap(~ neighbourhood_group_cleansed) +
  labs(title = 'Review Scores by Price',
       subtitle = 'With Applied log10 Transformation',
       x = 'Price',
       y = 'Review Score') +
  theme_bw()

# using guides(fill = FALSE) did not work to remove our legend, so we unfortunately had to go with theme()
# we understand this is not the best way to do it, but it seemed to work the best

a1 + theme(plot.title = element_text(size=15, face = 'bold'), legend.position = "none")

```


## Stop Words 

For this part of the analysis, we will take 20 listings from the top and bottom of the data, then sample another 20 randomly. We will then filter these through a list of stop words(deemed irrelevant to the data) to find relevant and unique words to describe these experiences.

``` {r stopWords}

# First, we'll define our array of stop words
stopWords <- c("airbnb", "a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "algo", "all", "almost", 
    "alone", "along", "alojen", "already", "also","although","always","am", "ammar","among", "amongst", "amoungst", "amount",  
    "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "aren't", "arrive", "arrived", "around", "as",  
    "at", "back", "bc", "be","became", "because", "becky", "become","becomes", "becoming", "been", "before", "beforehand", "behind", 
    "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", 
    "can", "cannot", "cant", "card","chaqueta", "chayda", "co", "con", "could", "couldnt", "de", "definitely", 
    "definetely", "dell", "defin", "describe", "detail", "did", "do", "doesn", "doesn't", "done", "don't", "down", "due", "during", 
    "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "episode", 
    "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", 
    "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", 
    "front", "full", "further", "get", "give", "go", "going", "had", "has", "hasnt", "have", "he", "hello", "hence", "her", 
    "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how",
    "however", "hundred", "i", "ie", "if", "i'll", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "it’s", "itself", "i've", "just",
    "keep", "la", "last", "latter", "latterly", "least", "leave", "less", "let's", "like", "ltd", "made", "many", "may", "me", "met", "meanwhile", "message",
    "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", 
    "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nsure", 
    "nor", "not", "nothing", "now", "nowhere", "nthe", "of", "off", "often", "on", "once", "one", "only", "onto", "or", 
    "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own", "para","part", "per", "perhaps", "place", 
    "please", "put", "rather", "re", "room", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", 
    "she", "should", "show", "side", "since", "six", "sixty", "so", "some", "somehow", "someone", 
    "something", "sometime", "sometimes", "somewhere", "still", "such", "take", "takes", "ten", "than", 
    "that", "thats", "the", "their", "them", "themselves", "the", "then", "thence", "there", "thereafter", "thereby", "therefore", 
    "therein", "thereupon", "these", "they", "thick", "thin", "third", "this", "those", "though", "three", 
    "through", "throughout", "thru", "thus", "to", "together", "too", "told", "top", "toward", "towards", "twelve", 
    "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "way", "we", "well", "were", 
    "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein",
    "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", 
    "why", "will", "with", "within", "without", "would", "year", "years", "yet", "you", "your", "you're", "yours", "yourself", "yourselves", "ve", "", " ")

# replace reviews listing id for join
reviews$id <- reviews$listing_id

# create a new joined frame with less data and grouped listings, for easier analysis
ordered <- listings %>%
  select(id, priceNum, neighbourhood_group_cleansed, review_scores_rating) %>%
  group_by(id, neighbourhood_group_cleansed) %>%
  filter(review_scores_rating == max(review_scores_rating)) %>%
  arrange(desc(review_scores_rating), desc(priceNum))

joined <- as.data.frame(left_join(ordered, reviews, by="id"))

# take samples of the data frame from the top, bottom, and (hopefully) middle
avg_review <- sample_n(joined, 20)
best_review <- head(joined, n=20)
worst_review <- tail(joined, n=20)

# Takes in a data frame and scrubs the review variable with regex - taking out punctuation, numbers and newlines
# NOTE: THIS DATA FRAME MUST HAVE A CHARACTER ATTRIBUTE 'COMMENTS'
filter_regex <- function(data) {
  
  # filter data with regex
  data$comments <- gsub("[^a-zA-Z ]","",data$comments, perl=TRUE, fixed = FALSE)
  
  # split strings and put frequencies in a table
  words <- data.frame(table(unlist(strsplit(tolower(data$comments), " "))))
  
  # name columns and remove stop words
  colnames(words) <- c("word", "count")
    
  words <- words %>% filter((!(words$word %in% stopWords)))
  
}

# Create our data frames by filtering reviews
best <- filter_regex(best_review)
avg <- filter_regex(avg_review)
worst <- filter_regex(worst_review)



```

## Reviews Word Analysis

The first correlation I noticed is the use of more abstract words against the expreience of each airbnb. When a place is better, words like "great", "easy", "flexible",  and "confidence" are used. The finding here is that a better experience may not be distinct in what makes it amazing, but rather there is a lack of inconvenience. It seems that people also value communication and accessibility in their AirBNB experience. "resposive", "accommodating", and many names are listed in these reviews. Perhaps there was a personal element to the stays that added to the experience. It is also worth noting that this is the only word cloud that easily fit into its box - people have a lot less to say when they are satisfied than when they are upset.

Average reviews were somewhere in the middle, where "clean", "available", and "comfortable" showed up often. These aren't generally descriptive of an exceptional experience, and moreso meet the baseline of a good AirBNB. Accessibility is still mentioned with "access", "communication", and "easy". Less names are mentioned in this data, but they are still present. Some people may have had better experiences when more names were mentioned. "Apartment" showed up more than any other word, as was the case with the top 20 graph. In New York City, apartments likely make up most properties. An average stay would probably be in an apartment(although some were in bungalows, apparently).

In our worst experiences, lots of red flags immediately come up. "Felons", "Smell", and "Homeless" are evidently bad. The learning here is the usage of less abstract words and more nouns. It seems specific things were present, and people have a lot to say about specific aspects of their stay. "Bathroom", "Room", "Water", "Basement" and "Smoke" showed up often - these things must contribute to negative experiences. People do not mention accessibility OR names, meaning the owners may not even be present for the experience. Where the best and average stays mentioned apartments, the worst experiences are "places".

Some interesting anomalies in the bad experiences include "sin", "knees", "cameras", "ghost", and "pathological" - one can easily imagine a bad scenario with these words.



```{r reviews, message=FALSE, warning=FALSE}

# Create some word clouds
w1 <- ggplot(data=best, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 8) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="Top 20 Reviews")

w2 <- ggplot(data=avg, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 12) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="20 Randomly Sampled Reviews")

w3 <- ggplot(data=worst, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 12) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="Worst 20 Reviews")

# a grid.arrange approach cut too much from the boxes, making analysis harder
w1
w2
w3

```

## Amenities by Price and Score

One characteristic outside of the price and review score we want to consider is the amenities offered by each Airbnb. As with the other graphs, it seems there isn't much of a correlation between the number of amenities and price or quality. The range of 15-40 amenities generally hold the bulk of the data, meaning that good Airbnb's generally have these many amenities. Some higher listings have upwards of 60 amenities, but it doesn't seem to make them much better than any of the other listings. A reason for this could be that the property owners have different ideas of what amenities mean, so the upper quantiles may have amenities like 'hot tubs' or 'lofts' while lower ones may have 'bathrooms' or 'bedrooms'. In further reports, diving deeper into the amenities themselves could provide a clearer picture of what makes an Airbnb better. The same case can be made with price - there is a diminishing return, but most listings fall within the 15-40 range for amenities.

```{r amenities, warning=FALSE, message=FALSE}

amens <- as.data.frame(sample_n(listings, 1500))


# This function takes in a list (with csv), and returns the length - the list may not be casted as such
# In the case of AirBNB, this is more efficient than modifying their amenities list to cast it as a list
listlen <- function(x) {
  length(strsplit(x, ",")[[1]])
}

# Apply the amenities counting function and store it in a new attribute
amens$amenCount <- as.numeric(lapply(amens$amenities, listlen))

a1 <- ggplot(data = amens,
       mapping = aes(x = amenCount, y=review_scores_rating)) +
  geom_point() +
  geom_smooth(method="lm") +
  scale_x_log10() +
  theme(plot.title = element_text(size= 15, face = 'bold')) +
  labs(title = 'Review Score by Number of Amenities',
       subtitle = 'Log 10 Scale on X',
       x = 'Number of Amenities',
       y = 'Review Score') +
  guides(fill = FALSE) +
  theme_bw()

a2 <- ggplot(data = amens %>% filter(priceNum < 2000),
       mapping = aes(x = amenCount, y=priceNum)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  theme(plot.title = element_text(size= 15, face = 'bold')) +
  labs(title = 'Price by Number of Amenities',
       subtitle= 'Prices > 2000 Removed for Easier Interpretation, Log 10 Scale on X',
       x = 'Number of Amenities',
       y = 'Price') +
  guides(fill = FALSE) +
  theme_bw()

grid.arrange(a1, a2)

```


# Machine Learning

```{r setup_ml, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(psych)
library(rpart)
library(rpart.plot)
library(lattice)
library(Cubist)
```

*For Machine Learning, we decide to use 7 variables (accuracy, cleanliness, check-in, communication, location, value, price) to try to explain the review score. The variables, other than price, are customers' separate ratings of Airbnb in these aspects. For example, 10 for location means the customer is satisfied with the location of that Airbnb. Also, the data we used included only Airbnb prices under $1,000. *
```{r setup}
scores <- listings %>% select("review_scores_rating", "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", "review_scores_location", "review_scores_value", "priceNum")

names(scores) <- c('Score', 'accuracy', 'cleanliness', 'checkin', 'communication', 'location', 'value', 'price')

scores <- scores %>% drop_na() %>% filter(price <= 1000)

summary(scores)
# pairs.panels(scores)
```

## Multiple Regression
We first use multiple regression to make the prediction for the entire data set. The correlation of prediction and true value is 0.89, which is pretty high correlation. Also, from the plot, we can tell that they do have a positive correlation. The mean average error (MAE) of the prediction is about 2.74. Since the total score is 100, a 2.74 margin of error is not too much.
From the multiple regression, we got **Score = -5.47 + 2.3 accuracy + 2.2 cleanliness + 0.86 checkin + 1.92 communication + 0.35 location + 2.77 value + 0.0025 price**
If customers are satisfied with Airbnb's cleanliness, communication, geographical location and other aspects, they will also be satisfied with the room overall.
```{r multiple reg}
score_lm <- lm(Score ~ . , data = scores)
score_lm
summary(score_lm)


pred1 <- predict(score_lm, scores)
cor(pred1, scores$Score)
plot(pred1, scores$Score)
abline(a = 0, b = 1, col = "red", lwd = 3, lty = 2)

MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}

mae_model <- MAE(pred1, scores$Score)
mae_model
mae_mean <- MAE(93.78, scores$Score)
mae_mean

mae_mean - mae_model
```

## Regression Tree

For Regression tree, we took 16620 observations as training data, and 10000 observations as testing data. 
From the plot, we can tell:
- If the value for Airbnb is less than 7.5, and the accuracy is less than 4.5, then it would get a review score about *29.8*.
- If the value for Airbnb is less than 7.5, the accuracy is greater than or equal to 4.5, and the communication is less than 6.5, then it would get a review score about *56.3*.
- If the value for Airbnb is less than 7.5, the accuracy is greater than or equal to 4.5, and the communication is greater than or equal to 6.5, then it would get a review score about *72.9*.
- If the value for Airbnb is greater than or equal to 7.5 and less than 8.5, and the accuracy is less than 9.5, then it would get a review score about *82.7*.
- If the value for Airbnb is greater than or equal to 8.5, and the accuracy is less than 9.5, then it would get a review score about *90.8*.
- If the value for Airbnb is greater than or equal to 7.5, the accuracy is greater than or equal to 9.5, and the cleanliness is less than 9.5, then it would get a review score about *94.4*.
- If the value for Airbnb is greater than or equal to 8, the accuracy is greater than or equal to 9.5, and the cleanliness is greater than or equal to 9.5, then it would get a highest review score *98*.

The mean average error (MAE) of the prediction is about 3.32.

```{r reg tree, warning=FALSE}
RNGversion('3.5.3')
set.seed(111)
train_sample <- sample(1:26620, 16620)

head(train_sample, 10)

train <- scores[train_sample, ]
test <- scores[-train_sample, ]

reg_tree <- rpart(Score ~. , data = train)
reg_tree

rpart.plot(reg_tree, digits = 3,
           fallen.leaves = TRUE,
           type = 4, extra = 101,
           box.palette = 'RdBu',
           shadow.col = 'gray')

pred2 <- predict(reg_tree, test)

summary(pred2)
summary(test$Score)

cor(pred2, test$Score)

MAE(pred2, test$Score)
```

## Model Tree
For the model tree, we used the same training and testing data and got 22 rules based on different situation. The correlation is 0.88 and the MAE is 2.64 which is the smallest MAE among these three models.
```{r modeltree}
model_tree <- cubist(x = train[-1],
                     y = train$Score)
summary(model_tree)

pred_tree <- predict(model_tree, test)

summary(pred_tree)
summary(test$Score)

cor(pred_tree, test$Score)

MAE(test$Score, pred_tree)
```

## Conclusions

New York City has a lot to do, and a lot of unique experiences per neighborhood and borough. We believe that some while some neighborhoods may not be as highly rated as others, the price you pay for an Airbnb is generally a good way to ensure a decent experience. Our qualitative analysis shows some diminishing return - meaning there is a point where spending more money will not make a better experience. An illustrative fact is that all five boroughs have a high average review score, yet the price increases significantly closer to Manhattan and Brooklyn. Of course, correlation and causation are not the same. Paying more for an Airbnb is NOT the reason it is good. Amenities usually have something to do with quality, meaning a good Airbnb usually has a decent amount of them.    

Because people generally use Airbnbs as a base for their vacation, we believe the biggest determinant in a good experience is communication. Most people simply want an owner that makes it easy to get in, drop their luggage and see the city. Worse AirBNB experiences usually come from necessities not being met - such as a dirty apartment or illegal happenings in or around the Airbnb. While people having a good experience often focus on things such as cleanliness and extra features, people with a bad experience notice the poor communication the most, and lack thereof.

The key points of this report point to location, accessibility, and communication.

If these basic needs are met at a baseline, we believe that a positive review and score will be given. A good Airbnb experience is a simple, accommodating apartment in a decent location - one that is not dirty or haunted.

## Limitations

This data was very thorough, expansive and full of opportunities for analysis. While some great conclusions were found, the size of the data presented a limitation. Several parts of the data were not computationally feasible for all 37k+ listings, so random samples that may not have been representative were taken. This study does not utilize many variables to find what correlates with positive reviews, such as bed count or particular accommodations. In the future, one could use more variables to see what neighborhoods or conditions make for a better experience. One could also analyze each neighborhood or borough independently. Manhattan may have has another 'level' of experience than the Bronx, so to speak. This is to say that a good Airbnb in the Bronx might be subpar in Manhattan. Analyzing what makes the experience better by neighborhood could be more individualized, as it could be based on different goals and variables. Finally, the number of amenities is not a qualitative analysis. Some owners may list basic necessities as amenities, while some random amenities may add more to the review score. Diving into these would surely give a better analysis.       

In the future, more machine learning could be applied to this analysis. A decision tree with multiple trials could be used to compare attributes or amenities, and to classify review scores as 'good', 'bad', or 'average'. This will give direction to what other attributes should be analyzed and reported on.