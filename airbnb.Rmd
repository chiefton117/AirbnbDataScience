---
title: "AirBNB Data Analysis"
author: "Tim Harrold, Madison Brown, Yifei Hao"
date: "3/10/2021"
output: html_document
---

In this analysis we will be observing qualitative and quantitative characteristics to discover the variation in Airbnbs throughout New York City and what makes the best Airbnb experience. We ask the question, which characteristics (variables) are correlated with a better reported experience. This analysis will include graphs to show the relationships between price and neighborhood, rating and neighborhood, rating and number of reviews, and rating and review stop words. We will be using tables, density plots, scatter plots, box plots, and maps to analyze the correlations between these variables. To give a qualitative analysis of peoples stays, we will be using word cloud to filter keywords from the reviews. A map of the city will provide spacial information on where the most vs least expensive Airbnbs are located in the city. We hypothesize that the Bronx and Brooklyn will have the higher densities of lower priced Airbnbs than Staten Island and Manhattan, and that there will be a positive correlation between price and review score. We hypothesize that the map will show the highest concentration of higher priced Airbnbs in Manhattan. In the cloud word, we hypothesize that PULL FROM TIMS DESCRIPTION. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(devtools)
library(maps)
library(leaflet)
library(ggmap)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggwordcloud)
library(grid)
library(gridExtra)

```

## Setting up the project

First, get our data from:

http://insideairbnb.com/get-the-data.html


To create these two files, right click the files listings.csv.gz and reviews.csv.gz separately, and click "extract here" or "extract to -> listings.csv", just like we extract the .zip files in class

Before committing and pushing, go to github desktop after you have the data files. Where it lists each file with a checkbox, right click the data files "listings.csv,reviews.csv,listings.csv.gz" and click:     
"ignore all .csv files(add to .gitignore)", then     
"ignore all .gz files(add to .gitignore)". 

If the data files are not removed from the list, right click them again and click "Ignore File(add to .gitignore). This will make it so you don't submit large data to the cloud. Github will not accept these large files(even the compressed ones) because of the size of our data.

```{r files}


listings <- read.csv('listings_NYC.csv');
reviews <- read.csv('reviews_NYC.csv');

```

## Density by Price

This graph show the density of Airbnbs at different prices within the five boroughs.  We hypothesized that the Bronx and Brooklyn would have the higher densities of lower priced Airbnbs than Staten Island and Manhattan. Our hypothesis is only partially correct. Brooklyn has a similar distribution to Staten Island, and queens has a similar distibution to the Bronx. Our hypothesis was supported in the case of the Bronx and Manhattan. This data show that the Bronx contain a higher distribution of lower priced Airbnbs than the other boroughs. Manhattan has the lowest density of lower priced Airbnbs. This is likely due to the difference in housing prices between Manhattan and the Bronx. 

```{r listings, echo=FALSE}

listings$priceNum <- as.numeric(gsub('[$,]', '', listings$price))

ggplot(data = listings %>% filter(priceNum < 400), mapping = aes(x=priceNum, fill=neighbourhood_group_cleansed)) +
  geom_density() +
  facet_wrap(~ neighbourhood_group_cleansed) +
  guides(fill = FALSE) +
  labs(title = "Airbnb Density by Price",
       x = 'Price',
       y = 'Density') +
  theme(plot.title = element_text(size= 15, face = 'bold'))

##ggplot(data = listings %>% filter(review_scores_rating > 50), mapping = aes(x=review_scores_rating, fill=neighbourhood_group_cleansed)) +
  ##geom_density() +
  ##facet_wrap(~ neighbourhood_group_cleansed)

```
## Review Scores by Price

This graph shows the review score by price of Airbnbs in each borough. We hypothesized that there would be a positive correlation between price and review score. The data show that higher priced Airbnbs tend to have high review scores. Most of the multithousand dollar Airbnbs do not have review scores below 80. Although more expensive Airbnbs trend towards higher scores, this does not mean that cheaper Airbnbs trend towards lower scores. The Airbnbs with the lower scores tend to be the cheaper ones; however, less expensive Airbnbs also have high scores. he data show that price has less impact on cheaper Airbnb reviews than expensive ones. 

```{r scatter, warning = FALSE}


ggplot(data = listings, mapping = aes(x=priceNum, y=review_scores_rating, color=neighbourhood_group_cleansed)) +
  geom_point() +
  facet_wrap(~ neighbourhood_group_cleansed) +
  guides(fill = FALSE) +
  labs(title = 'Review Scores by Price',
       x = 'Price',
       y = 'Review Score') +
  theme(plot.title = element_text(size=15, face = 'bold')) 


```
## Stop Words 



``` {r stopWords}

# First, we'll define our array of stop words
stopWords <- c("airbnb", "a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "algo", "all", "almost", 
    "alone", "along", "alojen", "already", "also","although","always","am", "ammar","among", "amongst", "amoungst", "amount",  
    "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "aren't", "arrive", "arrived", "around", "as",  
    "at", "back", "bc", "be","became", "because", "becky", "become","becomes", "becoming", "been", "before", "beforehand", "behind", 
    "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", 
    "can", "cannot", "cant", "card", "charline", "casper","chaqueta", "chayda", "christina", "co", "con", "could", "couldnt", "de", "definitely", "definetely", "dell", "defin",     "describe", "detail", "did", "do", "doesn", "doesn't", "done", "don't", 
    "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "episode", 
    "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", 
    "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", 
    "front", "full", "further", "get", "give", "go", "going", "had", "has", "hasnt", "have", "he", "hello", "hence", "her", 
    "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how",
    "however", "hundred", "i", "ie", "if", "i'll", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itâ€™s", "itself", "i've", "just",
    "keep", "la", "last", "latter", "latterly", "least", "leave", "less", "let's", "like", "ltd", "made", "many", "may", "me", "met", "meanwhile", "message",
    "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", 
    "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nsure", 
    "nor", "not", "nothing", "now", "nowhere", "nthe", "of", "off", "often", "on", "once", "one", "only", "onto", "or", 
    "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own", "para","part", "per", "perhaps", "place", 
    "please", "put", "rather", "re", "room", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", 
    "she", "should", "show", "side", "since", "six", "sixty", "so", "some", "somehow", "someone", 
    "something", "sometime", "sometimes", "somewhere", "still", "such", "take", "takes", "ten", "than", 
    "that", "thats", "the", "their", "them", "themselves", "the", "then", "thence", "there", "thereafter", "thereby", "therefore", 
    "therein", "thereupon", "these", "they", "thick", "thin", "third", "this", "those", "though", "three", 
    "through", "throughout", "thru", "thus", "to", "together", "too", "told", "top", "toward", "towards", "twelve", 
    "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "way", "we", "well", "were", 
    "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein",
    "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", 
    "why", "will", "with", "within", "without", "would", "year", "years", "yet", "you", "your", "you're", "yours", "yourself", "yourselves", "ve", "", " ")

# replace reviews listing id for join
reviews$id <- reviews$listing_id

# create a new joined frame with less data, for easier analysis
ordered <- listings %>%
  select(id, priceNum, neighbourhood_group_cleansed, review_scores_rating) %>%
  group_by(id, neighbourhood_group_cleansed) %>%
  filter(review_scores_rating == max(review_scores_rating)) %>%
  arrange(desc(review_scores_rating), desc(priceNum))

joined <- as.data.frame(left_join(ordered, reviews, by="id"))

avg_review <- sample_n(joined, 20)
best_review <- head(joined, n=20)
worst_review <- tail(joined, n=20)

# Takes in a data frame and scrubs the review variable with regex - taking out punctuation, numbers and newlines
filter_regex <- function(data) {
  

  # filter data with regex
  data$comments <- gsub("[^a-zA-Z ]","",data$comments, perl=TRUE, fixed = FALSE)


  words <- data.frame(table(unlist(strsplit(tolower(data$comments), " "))))
  
  colnames(words) <- c("word", "count")
    
  words <- words %>% filter((!(words$word %in% stopWords)))
  
}

# Create our data frames by filtering reviews

best <- filter_regex(best_review)
avg <- filter_regex(avg_review)
worst <- filter_regex(worst_review)



```

## Reviews Word Analysis

For this part of the analysis, I took 20 listings from the top and bottom of the data, then sampled another 20 randomly. I then filtered these through a list of stop words(deemed irrelevant to the data) to find 'unique' or 'relevant' words to these experiences.

The first correlation I noticed is the use of more abstract words against the expreience of each airbnb. When a place is better, words like "great", "easy", "flexible",  and "confidence" are used. The finding here is that a better experience may not be distinct in what makes it amazing, but rather there is a lack of inconvenience. It seems that people also value communication and accessibility in their AirBNB experience. "resposive", "accommodating", and many names are listed in these reviews. Perhaps there was a personal element to the stays that added to the experience.

Average reviews were somewhere in the middle, where "clean", "available", and "comfortable" showed up often. These aren't generally descriptive of an exceptional experience, and moreso meet the baseline of a good AirBNB. Accessibility is still mentioned with "access", "communication", and "easy". Less names are mentioned in this data, but they are still present. Some people may have had better experiences when more names were mentioned. "Apartment" showed up more than any other word, as was the case with the top 20 graph. In New York City, apartments likely make up most properties. An average stay would probably be in an apartment(although some were in bungalows, apparently).

In our worst experiences, lots of red flags immediately come up. "Felons", "Smell", and "Homeless" are evidently bad. The learning here is the usage of less abstract words and more nouns. It seems specific things were present, and people have a lot to say about specific aspects of their stay. "Bathroom", "Room", "Water", "Basement" and "Smoke" showed up often - these things must contribute to negative experiences. People do not mention accessibility OR names, meaning the owners may not even be present for the experience. Where the best and average stays mentioned apartments, the worst experiences are "places".

Some interesting anomalies in the bad experiences include "sin", "urine", "cameras", and "pathological" - One can easily imagine a bad scenario with these words.



```{r reviews, message=FALSE, warning=FALSE}

# Create some word clouds
w1 <- ggplot(data=best, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 8) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="Top 20 Reviews")

w2 <- ggplot(data=avg, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 12) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="20 Randomly Sampled Reviews")

w3 <- ggplot(data=worst, mapping=aes(label=word,size=count)) +
  geom_text_wordcloud(rm_outside = TRUE, area_corr_power = 1) +
  scale_size_area(max_size = 12) +
  theme_bw() +
  labs(title="NYC AirBnb Unique Word Frequency", subtitle="Worst 20 Reviews")


w1
w2
w3

```
## Box Plot of Price

(Working on this description- Madison)

```{r boxplot}
boxplot <- as.data.frame(sample_n(listings, 1500))

ggplot(data = boxplot %>% filter(priceNum < 2000), 
       mapping = aes(x = neighbourhood_group_cleansed, y = priceNum, fill = neighbourhood_group_cleansed)) +
  geom_boxplot() +
  theme(plot.title = element_text(size= 15, face = 'bold')) +
  labs(title = 'Price by Neighborhood',
       x = 'Neighborhood',
       y = 'Price') +
  guides(fill = FALSE) +
  theme_bw()
  

```

## Scatter plot of Ratings by Price

<<<<<<< Updated upstream
(Working on this Description- Madison)

```{r scatter, warning = FALSE}
set.seed(129)
scatter <- as.data.frame(sample_n(listings, 1000))

ggplot(data = listings, mapping = aes(x=priceNum, y=review_scores_rating, color=neighbourhood_group_cleansed)) +
  geom_point() +
  theme_bw() +
  labs(title = 'Ratings by Price',
       x = 'Review Score',
       y = 'Number of Reviews') +
   theme(plot.title = element_text(size= 15, face = 'bold'))
  facet_wrap(~ neighbourhood_group_cleansed) +
  theme_bw()


 
```

<<<<<<< Updated upstream
## Map of NYC (WHAT IS OUR VARIABLE?)

Show price and/or reviews in each neighborhood
=======
## Price Map

Here, we tried to utilize ggmaps and ggplot before resorting to leaflet for our map. I tried mapping the review scores, which didn't quite display right(most scores are actually pretty high!) even when mapping 68% of the data within one standard deviation of the mean.

The other challenge was taking a small enough subset of the data to map without crashing a computer(three computer crashes were involved in the making of this report). Leaflet offers multiple coloring methods, such as numerical, bin-based, and quantile-based. Quantiles ended up working especially well for this map.

This map

>>>>>>> Stashed changes
```{r maps, force = TRUE, message=FALSE, warning=FALSE}

set.seed(50)
subset <- sample_n(listings, 2000)

stdev <- sd(subset$priceNum)
mean <- mean(subset$priceNum)

# This is within one standard deviation of the mean
sData <- subset %>% filter(priceNum > (mean - stdev) & priceNum < (mean + stdev))
# This is within two standard deviations of the mean
sd2Data <- subset %>% filter(priceNum > mean - (2*stdev) & priceNum < mean + (2*stdev))

clrs <- colorQuantile(palette = "RdYlBu", domain = sData$priceNum, reverse = TRUE, na.color = NA)

# plot price on map, not sure what the actual markers signify?
leaflet(sData) %>%
  addTiles() %>%
  addCircleMarkers(data = sData,
           lng = ~longitude,
           lat = ~latitude,
           color = ~clrs(priceNum),
           stroke = FALSE, fillOpacity = 0.4, radius = 4) %>%
  addLegend("bottomright", pal = clrs, values = ~priceNum,
    title = "Price(Quantile)",
    labFormat = labelFormat(prefix = ""),
    opacity = 1) %>%
  addMarkers( clusterOptions = markerClusterOptions() ) %>%
  setView(-74.00, 40.71, zoom = 12) %>%
  addProviderTiles("CartoDB.Positron")


#clrsrev <- colorBin(palette = "RdYlBu", domain = sData$review_scores_rating, reverse = TRUE, na.color = NA)
clrsrev <- colorBin(palette = "RdYlBu", domain = sData$review_scores_rating, reverse = TRUE, na.color = NA)

# Where are most reviews found, and how much are they?
# Markers are stored on review scores < 60
leaflet(sData %>% filter(review_scores_rating < 60)) %>%
  addTiles() %>%
  addCircleMarkers(data = sData %>% filter(!is.na(review_scores_rating)),
           lng = ~longitude,
           lat = ~latitude,
           color = ~clrsrev(review_scores_rating),
           stroke = FALSE, fillOpacity = 0.4, radius = 4) %>%
  addLegend("bottomright", pal = clrsrev, values = ~review_scores_rating,
    title = "Review Score",
    opacity = 1) %>%
  addMarkers( clusterOptions = markerClusterOptions() ) %>%
  setView(-74.00, 40.71, zoom = 12) %>%
  addProviderTiles("CartoDB.Positron")






```
